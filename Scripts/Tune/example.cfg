[DEFAULT]
# Path to your data generated using ShuffleSplit.py
# Should include all HDF5 and csv files
data path = /users/kmonaghan/caffe/Automation/
# Where to output the results from the networks
output path = /users/kmonaghan/caffe/Automation/results.csv
# Path to your solver file (One is included in the repo)
solver path = /users/kmonaghan/caffe/Automation/solver.prototxt
# Path where caffe will store snapshots and prototxts
temp path = /users/kmonaghan/caffe/Working
name = TestNet
learning rate = 0.01
weight decay = 0.0005
# The epoch settings controls the number of times the network will see each data point in the training set
# 200 epochs means the network will be trained on each data point 200 times
epochs = 200
# Evaluate the network every 50 epochs
test interval = 50
# Number of different labels.  Since the example is binary, there are two classes
number of classes = 2
# Number of samples to pass through the network at once
batch size = 512
# Average the results across 10 different train/test splits
folds = 10

# Network #1
[Net1]
# 1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
# Number of neurons in each layer, separated by commas
network = 50,50
# Percentage of nuerons to dropout during training phase (input layer)
input dropout = 0.1
# Percentage of nuerons to dropout during training phase (hidden layers)
hidden dropout = 0.5
# 1 = Xavier, 2 = Gaussian
filler = 1
l2 regularization = False

# Network #2
[Net2]
# 1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
network = 50,50,50
input dropout = 0.1
hidden dropout = 0.5
# 1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False










